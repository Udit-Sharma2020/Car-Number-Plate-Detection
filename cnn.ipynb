{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mxFumQmLwCQb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# can use the below import should you choose to initialize the weights of your Net\n",
    "import torch.nn.init as I\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Covolutional Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 2)\n",
    "\n",
    "        # Maxpooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(in_features = 230400, out_features = 1000) # The number of input gained by \"print(\"Flatten size: \", x.shape)\" in below\n",
    "        self.fc2 = nn.Linear(in_features = 1000,    out_features = 1000)\n",
    "        self.fc3 = nn.Linear(in_features = 1000,    out_features = 1) # the output 136 in order to having 2 for each of the 68 keypoint (x, y) pairs\n",
    "\n",
    "        # Dropouts\n",
    "        self.drop1 = nn.Dropout(p = 0.1)\n",
    "        self.drop2 = nn.Dropout(p = 0.2)\n",
    "        self.drop3 = nn.Dropout(p = 0.3)\n",
    "        self.drop4 = nn.Dropout(p = 0.4)\n",
    "        self.drop5 = nn.Dropout(p = 0.5)\n",
    "        self.drop6 = nn.Dropout(p = 0.6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First - Convolution + Activation + Pooling + Dropout\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop1(x)\n",
    "        #print(\"First size: \", x.shape)\n",
    "\n",
    "        # Second - Convolution + Activation + Pooling + Dropout\n",
    "        x = self.drop2(self.pool(F.relu(self.conv2(x))))\n",
    "        #print(\"Second size: \", x.shape)\n",
    "\n",
    "        # Third - Convolution + Activation + Pooling + Dropout\n",
    "        x = self.drop3(self.pool(F.relu(self.conv3(x))))\n",
    "        #print(\"Third size: \", x.shape)\n",
    "\n",
    "        # Forth - Convolution + Activation + Pooling + Dropout\n",
    "        x = self.drop4(self.pool(F.relu(self.conv4(x))))\n",
    "        #print(\"Forth size: \", x.shape)\n",
    "\n",
    "        # Flattening the layer\n",
    "        x = x.flatten()\n",
    "        #print(\"Flatten size: \", x.shape)\n",
    "\n",
    "        # First - Dense + Activation + Dropout\n",
    "        x = self.drop5(F.relu(self.fc1(x)))\n",
    "        #print(\"First dense size: \", x.shape)\n",
    "\n",
    "        # Second - Dense + Activation + Dropout\n",
    "        x = self.drop6(F.relu(self.fc2(x)))\n",
    "        #print(\"Second dense size: \", x.shape)\n",
    "\n",
    "        # Final Dense Layer\n",
    "        x = self.fc3(x)\n",
    "        #print(\"Final dense size: \", x.shape)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
